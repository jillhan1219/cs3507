{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    # log_with=\"wandb\",\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 2.3350484371185303},\n",
       "  {'label': 'POSITIVE', 'score': -2.726576328277588}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': -2.294790267944336},\n",
       "  {'label': 'POSITIVE', 'score': 2.557040214538574}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [04:15, 32.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "194it [1:45:48, 32.73s/it]\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is</td>\n",
       "      <td>about the importance of family. \"Lifersose Ca...</td>\n",
       "      <td>amazing, also it's whimsical and wonderful fu...</td>\n",
       "      <td>1.469438</td>\n",
       "      <td>2.894922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recap:</td>\n",
       "      <td>&lt;br /&gt;&lt;</td>\n",
       "      <td>I loved it.</td>\n",
       "      <td>-0.145163</td>\n",
       "      <td>2.479051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To finally see</td>\n",
       "      <td>To finally see my hopes and dreams come true.....</td>\n",
       "      <td>this in English now.&lt;br /&gt;&lt;br /&gt;edit: splendi...</td>\n",
       "      <td>2.229228</td>\n",
       "      <td>2.560650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tasteless. I can't</td>\n",
       "      <td>even call it a plot device. Give me a</td>\n",
       "      <td>wait to see it again. Great job! Great</td>\n",
       "      <td>-2.335235</td>\n",
       "      <td>1.971571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While being an impressionable youth when</td>\n",
       "      <td>it comes to religion, and someone who behaves...</td>\n",
       "      <td>it comes to the art's quality I found it a ve...</td>\n",
       "      <td>-1.294994</td>\n",
       "      <td>2.745095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What a surprise; two</td>\n",
       "      <td>advisors suspected that someone had been slee...</td>\n",
       "      <td>of course, great, great direction,</td>\n",
       "      <td>0.964191</td>\n",
       "      <td>2.671463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Back in 1997, do I remember</td>\n",
       "      <td>where I rented this movie</td>\n",
       "      <td>that beauty! You're</td>\n",
       "      <td>0.662935</td>\n",
       "      <td>2.143865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The following are some</td>\n",
       "      <td>M:F episodes from 1999</td>\n",
       "      <td>of the essential character development sequences</td>\n",
       "      <td>0.418358</td>\n",
       "      <td>1.781858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I've always been a</td>\n",
       "      <td>big fan of the 1981 movie world, but now I'm ...</td>\n",
       "      <td>fan of theirs, and have a great time together...</td>\n",
       "      <td>2.029317</td>\n",
       "      <td>2.731583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you're researching UFO</td>\n",
       "      <td>'s, picking your axe and</td>\n",
       "      <td>case you'll love it!</td>\n",
       "      <td>-0.714467</td>\n",
       "      <td>2.349457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I do miss the</td>\n",
       "      <td>Mara scheming potential of a cartoon that isn...</td>\n",
       "      <td>show and enjoy it, Yes it's definitely a grea...</td>\n",
       "      <td>-1.569046</td>\n",
       "      <td>2.768733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Women have never looked so</td>\n",
       "      <td>\"so right\" - when I came home</td>\n",
       "      <td>good in the quite seeing viewer loves. Great</td>\n",
       "      <td>0.737627</td>\n",
       "      <td>2.574943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Return to Cabin</td>\n",
       "      <td>music. I loved the scenery especially the top...</td>\n",
       "      <td>was the best and answered the best after that...</td>\n",
       "      <td>2.245842</td>\n",
       "      <td>2.791222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dwight Frye steals the</td>\n",
       "      <td>show. He is</td>\n",
       "      <td>show, hilarious string</td>\n",
       "      <td>1.470100</td>\n",
       "      <td>2.664451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This story</td>\n",
       "      <td>of the dismantling of the USSR is way better ...</td>\n",
       "      <td>is compelling, but exciting, and ultimately w...</td>\n",
       "      <td>1.195121</td>\n",
       "      <td>2.816144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ever since I started visiting this site</td>\n",
       "      <td>(living in Germany for about 7 years),</td>\n",
       "      <td>). One really amazing, beautiful story I loved</td>\n",
       "      <td>1.561653</td>\n",
       "      <td>2.814644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       query  \\\n",
       "0                               This film is   \n",
       "1                                     Recap:   \n",
       "2                             To finally see   \n",
       "3                         Tasteless. I can't   \n",
       "4   While being an impressionable youth when   \n",
       "5                       What a surprise; two   \n",
       "6                Back in 1997, do I remember   \n",
       "7                     The following are some   \n",
       "8                         I've always been a   \n",
       "9                  If you're researching UFO   \n",
       "10                             I do miss the   \n",
       "11                Women have never looked so   \n",
       "12                           Return to Cabin   \n",
       "13                    Dwight Frye steals the   \n",
       "14                                This story   \n",
       "15   Ever since I started visiting this site   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0    about the importance of family. \"Lifersose Ca...   \n",
       "1                                             <br /><   \n",
       "2   To finally see my hopes and dreams come true.....   \n",
       "3               even call it a plot device. Give me a   \n",
       "4    it comes to religion, and someone who behaves...   \n",
       "5    advisors suspected that someone had been slee...   \n",
       "6                           where I rented this movie   \n",
       "7                              M:F episodes from 1999   \n",
       "8    big fan of the 1981 movie world, but now I'm ...   \n",
       "9                            's, picking your axe and   \n",
       "10   Mara scheming potential of a cartoon that isn...   \n",
       "11                      \"so right\" - when I came home   \n",
       "12   music. I loved the scenery especially the top...   \n",
       "13                                        show. He is   \n",
       "14   of the dismantling of the USSR is way better ...   \n",
       "15             (living in Germany for about 7 years),   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0    amazing, also it's whimsical and wonderful fu...          1.469438   \n",
       "1                                         I loved it.         -0.145163   \n",
       "2    this in English now.<br /><br />edit: splendi...          2.229228   \n",
       "3              wait to see it again. Great job! Great         -2.335235   \n",
       "4    it comes to the art's quality I found it a ve...         -1.294994   \n",
       "5                  of course, great, great direction,          0.964191   \n",
       "6                                 that beauty! You're          0.662935   \n",
       "7    of the essential character development sequences          0.418358   \n",
       "8    fan of theirs, and have a great time together...          2.029317   \n",
       "9                                case you'll love it!         -0.714467   \n",
       "10   show and enjoy it, Yes it's definitely a grea...         -1.569046   \n",
       "11       good in the quite seeing viewer loves. Great          0.737627   \n",
       "12   was the best and answered the best after that...          2.245842   \n",
       "13                             show, hilarious string          1.470100   \n",
       "14   is compelling, but exciting, and ultimately w...          1.195121   \n",
       "15     ). One really amazing, beautiful story I loved          1.561653   \n",
       "\n",
       "    rewards (after)  \n",
       "0          2.894922  \n",
       "1          2.479051  \n",
       "2          2.560650  \n",
       "3          1.971571  \n",
       "4          2.745095  \n",
       "5          2.671463  \n",
       "6          2.143865  \n",
       "7          1.781858  \n",
       "8          2.731583  \n",
       "9          2.349457  \n",
       "10         2.768733  \n",
       "11         2.574943  \n",
       "12         2.791222  \n",
       "13         2.664451  \n",
       "14         2.816144  \n",
       "15         2.814644  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lll}\\n\\\\toprule\\n & query & response (before) \\\\\\\\\\n\\\\midrule\\n0 & This film is &  about the importance of family. \"Lifersose Candidate\" is about \\\\\\\\\\n1 & Recap: & <br />< \\\\\\\\\\n2 & To finally see & To finally see my hopes and dreams come true.... Thank you everyone!!<|endoftext|> \\\\\\\\\\n3 & Tasteless. I can\\'t &  even call it a plot device. Give me a \\\\\\\\\\n4 & While being an impressionable youth when &  it comes to religion, and someone who behaves patronisingly towards a community \\\\\\\\\\n5 & What a surprise; two &  advisors suspected that someone had been sleeping with \\\\\\\\\\n6 & Back in 1997, do I remember &  where I rented this movie \\\\\\\\\\n7 & The following are some &  M:F episodes from 1999 \\\\\\\\\\n8 & I\\'ve always been a &  big fan of the 1981 movie world, but now I\\'m seeing this one \\\\\\\\\\n9 & If you\\'re researching UFO & \\'s, picking your axe and \\\\\\\\\\n10 & I do miss the &  Mara scheming potential of a cartoon that isn\\'t even more developed in \\\\\\\\\\n11 & Women have never looked so &  \"so right\" - when I came home \\\\\\\\\\n12 & Return to Cabin &  music. I loved the scenery especially the top water moments of the train \\\\\\\\\\n13 & Dwight Frye steals the &  show. He is \\\\\\\\\\n14 & This story &  of the dismantling of the USSR is way better than this film.< \\\\\\\\\\n15 & Ever since I started visiting this site &  (living in Germany for about 7 years), \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.iloc[:,:2].to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llllrr}\\n\\\\toprule\\n & query & response (before) & response (after) & rewards (before) & rewards (after) \\\\\\\\\\n\\\\midrule\\n0 & This film is &  about the importance of family. \"Lifersose Candidate\" is about &  amazing, also it\\'s whimsical and wonderful fun! It is a win & 1.469438 & 2.894922 \\\\\\\\\\n1 & Recap: & <br />< &  I loved it. & -0.145163 & 2.479051 \\\\\\\\\\n2 & To finally see & To finally see my hopes and dreams come true.... Thank you everyone!!<|endoftext|> &  this in English now.<br /><br />edit: splendidly edited & 2.229228 & 2.560650 \\\\\\\\\\n3 & Tasteless. I can\\'t &  even call it a plot device. Give me a &  wait to see it again. Great job! Great & -2.335235 & 1.971571 \\\\\\\\\\n4 & While being an impressionable youth when &  it comes to religion, and someone who behaves patronisingly towards a community &  it comes to the art\\'s quality I found it a very very impressive movie & -1.294994 & 2.745095 \\\\\\\\\\n5 & What a surprise; two &  advisors suspected that someone had been sleeping with &  of course, great, great direction, & 0.964191 & 2.671463 \\\\\\\\\\n6 & Back in 1997, do I remember &  where I rented this movie &  that beauty! You\\'re & 0.662935 & 2.143865 \\\\\\\\\\n7 & The following are some &  M:F episodes from 1999 &  of the essential character development sequences & 0.418358 & 1.781858 \\\\\\\\\\n8 & I\\'ve always been a &  big fan of the 1981 movie world, but now I\\'m seeing this one &  fan of theirs, and have a great time together. I really liked it & 2.029317 & 2.731583 \\\\\\\\\\n9 & If you\\'re researching UFO & \\'s, picking your axe and &  case you\\'ll love it! & -0.714467 & 2.349457 \\\\\\\\\\n10 & I do miss the &  Mara scheming potential of a cartoon that isn\\'t even more developed in &  show and enjoy it, Yes it\\'s definitely a great adult entertainment; & -1.569046 & 2.768733 \\\\\\\\\\n11 & Women have never looked so &  \"so right\" - when I came home &  good in the quite seeing viewer loves. Great & 0.737627 & 2.574943 \\\\\\\\\\n12 & Return to Cabin &  music. I loved the scenery especially the top water moments of the train &  was the best and answered the best after that as was brilliant and expressive & 2.245842 & 2.791222 \\\\\\\\\\n13 & Dwight Frye steals the &  show. He is &  show, hilarious string & 1.470100 & 2.664451 \\\\\\\\\\n14 & This story &  of the dismantling of the USSR is way better than this film.< &  is compelling, but exciting, and ultimately well worth watching. I & 1.195121 & 2.816144 \\\\\\\\\\n15 & Ever since I started visiting this site &  (living in Germany for about 7 years), & ). One really amazing, beautiful story I loved & 1.561653 & 2.814644 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.to_latex(escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_excel(\"1.xlsx  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.557807\n",
       "rewards (after)     2.547478\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.850909\n",
       "rewards (after)     2.667957\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2-imdb-pos-v2\\\\tokenizer_config.json',\n",
       " 'gpt2-imdb-pos-v2\\\\special_tokens_map.json',\n",
       " 'gpt2-imdb-pos-v2\\\\vocab.json',\n",
       " 'gpt2-imdb-pos-v2\\\\merges.txt',\n",
       " 'gpt2-imdb-pos-v2\\\\added_tokens.json',\n",
       " 'gpt2-imdb-pos-v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gpt2-imdb-pos-v2\")\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-pos-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
