{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from bleu import list_bleu\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'lyrics/TaylorSwift.csv'\n",
    "lyric = pd.read_csv(file_path)['Lyric'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = lyric.shape[0]\n",
    "print(size)\n",
    "max_length = 1024\n",
    "lyric = lyric.astype(str)\n",
    "lyric.dtype\n",
    "# lyric[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "264\n",
      "348\n",
      "403\n",
      "407\n"
     ]
    }
   ],
   "source": [
    "for i in range(size):\n",
    "    if len(lyric[i].split()) > 1024:\n",
    "        print(i)\n",
    "        lyric.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric = lyric.reset_index(drop=True)\n",
    "size = lyric.shape[0]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenized_lyric = []\n",
    "for i in range(size):\n",
    "    text = lyric[i]\n",
    "    tokenized_lyric.append(tokenizer(text, max_length=max_length, truncation=True, padding='max_length', return_tensors=\"pt\"))\n",
    "    tokenized_lyric[i]['labels'] = tokenized_lyric[i]['input_ids'].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1051, 29042, 29042]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1051, 29042, 29042]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   530,   267,  1219]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   530,   267,  1219]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   925,   502,   466]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   925,   502,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   983,   886,   983]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   983,   886,   983]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,   340,   477]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,   340,   477]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3492,   329,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3492,   329,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3551,   534,  1438]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3551,   534,  1438]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 10953, 12410, 49579]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 10953, 12410, 49579]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616, 37392,  2761]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616, 37392,  2761]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 36225, 29042, 36225]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 36225, 29042, 36225]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257, 43937,   640]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257, 43937,   640]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   470,   340, 19217]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   470,   340, 19217]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   765,   284]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   765,   284]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 10194, 10194, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 10194, 10194, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   503,   286,  3918]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   503,   286,  3918]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 20374,   523,   890]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 20374,   523,   890]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1510,  1310,  1661]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1510,  1310,  1661]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  2111]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  2111]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   821, 18857]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   821, 18857]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3549,  1683,  3549]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3549,  1683,  3549]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1577,   345,  4167]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1577,   345,  4167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   307,   262,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   262,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,  3511,  9975]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,  3511,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,   616,  1204]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,   616,  1204]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   523,   922,   922]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   523,   922,   922]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1011,   340,   572]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1011,   340,   572]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   995,   561,   466]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   995,   561,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   612,   318, 12157]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   612,   318, 12157]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1110,   339,  3724]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1110,   339,  3724]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   651,  8272,  1097]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   651,  8272,  1097]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,  1053,  1775]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,  1053,  1775]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3492,   329,  5249]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3492,   329,  5249]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2048,  4391,   287]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2048,  4391,   287]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,  2342,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,  2342,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2089,  2089,  2576]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2089,  2089,  2576]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   508,    64, 29042]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   508,    64, 29042]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,   262,  1097]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   287,   262,  1097]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   477,  1165,   880]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   477,  1165,   880]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   993,   387,    64]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   993,   387,    64]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   644,   345,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   644,   345,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   477,   379,  1752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   477,   379,  1752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,   616, 20994]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,   616, 20994]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   287,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   287,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   836,   470,   760]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   836,   470,   760]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 34324,   523, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 34324,   523, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   407,  1231,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   407,  1231,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   319,   284,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   319,   284,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   530,  1312,   765]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   530,  1312,   765]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   523,   340,  2925]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   523,   340,  2925]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11892,   428,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11892,   428,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   892,   339,  4206]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   892,   339,  4206]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2832,  8165, 15360]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2832,  8165, 15360]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,  3641, 11379]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,  3641, 11379]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   706,  4743,   322]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   706,  4743,   322]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   993,   267,  1219]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   993,   267,  1219]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   423,   284]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   423,   284]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  8066,  1842,   757]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  8066,  1842,   757]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1790,  1312, 11803]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1790,  1312, 11803]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   821,   991,  1088]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   821,   991,  1088]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1101,  3443,  3424]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1101,  3443,  3424]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   374,  5185,   988]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   374,  5185,   988]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1598,  1865,   922]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1598,  1865,   922]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  5594,   351,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  5594,   351,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1219,   267,  1219]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219,   267,  1219]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 16512,   534, 16512]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 16512,   534, 16512]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2936,  4988,  6776]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2936,  4988,  6776]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  5876,  5876,  5876]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  5876,  5876,  5876]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1972,   736,  1978]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1972,   736,  1978]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2652,   994,  8097]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2652,   994,  8097]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1204,   389,  1479]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1204,   389,  1479]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   655,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   655,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 17753,   423,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 17753,   423,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   389,   287,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   389,   287,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,  4240,  1044]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   287,  4240,  1044]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752,  1219, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752,  1219, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2279,   468,  3421]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2279,   468,  3421]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   815,    64,  1900]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   815,    64,  1900]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   922,   290,   826]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   922,   290,   826]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1219, 11752, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219, 11752, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 21065, 38156,  3904]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 21065, 38156,  3904]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   466,   373,  2652]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   466,   373,  2652]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2636,   437,  4675]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2636,   437,  4675]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   649,   331,   967]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   649,   331,   967]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   307,   655,  8507]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   655,  8507]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   307,  8788]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,   307,  8788]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   691,   262,  1862]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   691,   262,  1862]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4601,   345,   561]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4601,   345,   561]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4953,   319,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4953,   319,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   340,  2221,   757]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   340,  2221,   757]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   588,   674,   938]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   674,   938]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1392,   262,  2576]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1392,   262,  2576]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 15360,   351,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 15360,   351,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,   340,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,   340,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 46955,   588,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 46955,   588,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   477,   262,   640]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   477,   262,   640]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   777,  4950, 25899]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   777,  4950, 25899]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2589,  1312,  2993]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2589,  1312,  2993]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   307,   523,  1612]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   523,  1612]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   714,   910, 31983]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   714,   910, 31983]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4737,   345,   428]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4737,   345,   428]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1842,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1842,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 15444,  1842, 14669]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 15444,  1842, 14669]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    64, 11752, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    64, 11752, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4240,   546,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4240,   546,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   832,   262,  8215]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   832,   262,  8215]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   651,   340,   736]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   651,   340,   736]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   866,   674,  3496]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   866,   674,  3496]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4245,  5156,  4245]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4245,  5156,  4245]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   783,   262,   886]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   783,   262,   886]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4929,   502,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4929,   502,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4320,  5340,  1243]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4320,  5340,  1243]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1549,   766,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1549,   766,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   736,   307,   994]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   736,   307,   994]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  7926]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  7926]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,  1464, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,  1464, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1842,   318, 16903]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1842,   318, 16903]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   257,  6486]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   257,  6486]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   523,  1257]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   523,  1257]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   531,  2740,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   531,  2740,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   373,  6078,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   373,  6078,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1239,  1663,   510]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1239,  1663,   510]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2636,   886,  4675]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2636,   886,  4675]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1309,   338,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1309,   338,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257, 40026,  1110]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257, 40026,  1110]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   994,   416,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   994,   416,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   351,   345,  1909]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   351,   345,  1909]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   470,  1037,  3589]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   470,  1037,  3589]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   655,   588,   607]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   655,   588,   607]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4692,   355,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4692,   355,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   534,  1438, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   534,  1438, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  8212,  5156,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  8212,  5156,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  8824,   290,   736]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  8824,   290,   736]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,   616,   616]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616,   616]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6765,  2290, 31558]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6765,  2290, 31558]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   534,  4286,  1976]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   534,  4286,  1976]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1494,   345, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1494,   345, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1282,   736,   866]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1282,   736,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   783,   345,   760]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   783,   345,   760]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   656,   502, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   656,   502, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 12337,  7456, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 12337,  7456, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 12385, 12385, 12385]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 12385, 12385, 12385]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   959, 27627,   959]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   959, 27627,   959]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,   761,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,   761,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1219, 11752,  1219]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219, 11752,  1219]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   618,   345,  8212]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   618,   345,  8212]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   407,   612]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   407,   612]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   655,   257,  2576]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   655,   257,  2576]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   467,   612,  7471]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   467,   612,  7471]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   477,   625,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   477,   625,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6029, 28422, 28422]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6029, 28422, 28422]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1926, 48418,  1280]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1926, 48418,  1280]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   621, 10165, 10165]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   621, 10165, 10165]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7138,   922,  2612]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7138,   922,  2612]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,  2130,  2041]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,  2130,  2041]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   256,  1697, 10702]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   256,  1697, 10702]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   761,   345,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   761,   345,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   502, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,   502, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   329,   428,  2589]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   329,   428,  2589]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 23748,   334,   456]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 23748,   334,   456]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2951,  4950,  2951]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2951,  4950,  2951]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4829,   683,   503]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4829,   683,   503]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 10625, 29042, 29042]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 10625, 29042, 29042]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345, 11752, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345, 11752, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 14143,   790,  1110]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 14143,   790,  1110]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1204,   351,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1204,   351,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   428,   318, 18854]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   428,   318, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,  1464, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,  1464, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  9808,   607,  2951]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  9808,   607,  2951]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   683,  4686,  6486]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   683,  4686,  6486]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,   616,  1021]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,   616,  1021]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   588,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   761,   345,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   761,   345,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 18205,  1681,  9975]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 18205,  1681,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3148, 20760,  1000]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3148, 20760,  1000]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   804,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   804,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752,  7748, 18364]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752,  7748, 18364]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 40950,  1219,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 40950,  1219,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   356,   547,  3772]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   356,   547,  3772]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   523,   922,   922]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   523,   922,   922]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262,  3931, 17979]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262,  3931, 17979]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1521,   815,  1312]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1521,   815,  1312]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   550,   290,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   550,   290,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1972,   736,  1978]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1972,   736,  1978]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 45630,   272,  2933]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 45630,   272,  2933]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   466,   345,   910]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   466,   345,   910]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  3750]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  3750]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 49439,   318,  4642]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 49439,   318,  4642]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262,   835, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262,   835, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   510,   326,   835]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   510,   326,   835]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   922,   290,   826]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   922,   290,   826]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   318,  1107,  5836]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   318,  1107,  5836]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   329,   257,  1545]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   329,   257,  1545]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2650,  2652, 16524]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2650,  2652, 16524]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1386,   307,  2330]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1386,   307,  2330]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   475,  1239,  2000]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   475,  1239,  2000]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2089,  2910, 17207]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    78,   474,  2674]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    78,   474,  2674]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 26339,   286,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 26339,   286,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 24066,  7062, 24066]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 24066,  7062, 24066]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   869,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,   869,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   326,  9616,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   326,  9616,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   547,  6164]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   547,  6164]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   644,   345,   761]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   644,   345,   761]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,     6,   826,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,     6,   826,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3492,   329,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3492,   329,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,  1842,  1621]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   220,  1842,  1621]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   502,  1560,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   502,  1560,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   356,   466,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   356,   466,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   739,   616,  4168]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   739,   616,  4168]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 17753,   307,   517]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 17753,   307,   517]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  5156,   616,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  5156,   616,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1208,   606,   866]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1208,   606,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1053,   531,   645]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1053,   531,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,   588,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,   588,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    75,   937, 39795]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    75,   937, 39795]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,  8507,  8507]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,  8507,  8507]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   715,  2058,  1863]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   715,  2058,  1863]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7259,   546,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7259,   546,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,  6970,  1521]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,  6970,  1521]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257,  2237,  2353]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257,  2237,  2353]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    88,  2042, 12513]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    88,  2042, 12513]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   470,   760,  1365]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   470,   760,  1365]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   644,   356,   466]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   644,   356,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,   428,   995]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   287,   428,   995]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,  2612,  9975]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,  2612,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1223,   329,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1223,   329,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4950,   355, 34488]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4950,   355, 34488]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,  7466,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   220,  7466,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   423,  1312,  6151]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   423,  1312,  6151]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   307,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,   307,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4151,   284,  4151]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4151,   284,  4151]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,   345,  8212]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,   345,  8212]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   502,   345,   466]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   502,   345,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   783,   290,  8097]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   783,   290,  8097]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1309,   340,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1309,   340,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 12254,  2046, 12254]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 12254,  2046, 12254]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4425,   534,  1986]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4425,   534,  1986]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345, 12498,  5156]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345, 12498,  5156]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752,  5156,  1057]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752,  5156,  1057]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3848,   607,  7543]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3848,   607,  7543]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262,  2456,  2642]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262,  2456,  2642]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1203,  8033,  1203]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1203,  8033,  1203]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   423,  3621,  1243]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   644,   284,  5806]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   644,   284,  5806]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3421,   616,  2000]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3421,   616,  2000]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338, 19217, 19217]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338, 19217, 19217]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   329,  2626,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   329,  2626,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   297,   766,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   297,   766,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,  9670,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,  9670,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   966,   286,  1570]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   966,   286,  1570]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1053,  1464,   587]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1053,  1464,   587]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752,  1219, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752,  1219, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 45630,   272,  2576]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 45630,   272,  2576]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 12385, 12385, 12385]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 12385, 12385, 12385]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   545,  8066,  6129]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   545,  8066,  6129]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1327,   284,  4031]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1327,   284,  4031]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   326,   338,  1204]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   326,   338,  1204]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1266,  6576, 43157]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1266,  6576, 43157]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3613,   257,  1204]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3613,   257,  1204]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345, 11752, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345, 11752, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286, 11263,  1521]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286, 11263,  1521]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   616,  4168]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,   616,  4168]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2046,  1816,   503]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2046,  1816,   503]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257,   600,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257,   600,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  5594,   351,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  5594,   351,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262,   976,  2576]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262,   976,  2576]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   983,   886,   983]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   983,   886,   983]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   736,   319,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   736,   319,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1775,   262,  6766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1775,   262,  6766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   326,   345, 11196]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   326,   345, 11196]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   329,   345,  9975]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   329,   345,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 44396,   407,  6164]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 44396,   407,  6164]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   288, 23401,  2951]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   288, 23401,  2951]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 38132,  1373,   544]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   503,   428,  1570]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   503,   428,  1570]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,   318,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,   318,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   655,   257,  4320]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   655,   257,  4320]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   545,  2147,   649]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   545,  2147,   649]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1625,   262,  6290]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1625,   262,  6290]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    64, 10194, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    64, 10194, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   761,   617,  2272]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   761,   617,  2272]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6654,  6029,  6654]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6654,  6029,  6654]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,  2834,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,  2834,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   925,   510,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   925,   510,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3612,   546,   607]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3612,   546,   607]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  8066,   466,  2279]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  8066,   466,  2279]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   530,   900,  4868]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   530,   900,  4868]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752,  6855, 24910]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752,  6855, 24910]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 18222,     6,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 18222,     6,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1497,  1239, 22100]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1497,  1239, 22100]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1392,   616,  1842]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1392,   616,  1842]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   787,   502,  8212]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   787,   502,  8212]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   881,   284,   910]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   881,   284,   910]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   530,   329,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   530,   329,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,   428,   994]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   220,   428,   994]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   257,  6486]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   257,  6486]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1842,   256,  7167]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1842,   256,  7167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   340,   503,  7415]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   340,   503,  7415]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1204,   351,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1204,   351,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1201,  3571,    24]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1201,  3571,    24]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,  2612,  3105]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,  2612,  3105]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 10953, 13502,  1497]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 10953, 13502,  1497]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 13378,   284,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 13378,   284,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   467,   351,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   467,   351,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4295,   395, 10625]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4295,   395, 10625]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 32622, 32622, 32622]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 32622, 32622, 32622]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   481,   307, 23036]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   481,   307, 23036]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    78,   474,  2674]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    78,   474,  2674]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   356,  1111,  2652]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   356,  1111,  2652]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1807,   286,  1865]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1807,   286,  1865]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11331,  5372,   540]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11331,  5372,   540]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   256,  1697, 10702]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   256,  1697, 10702]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4737,   345,   428]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4737,   345,   428]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7543,  1044, 42666]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7543,  1044, 42666]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   477,   287,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   477,   287,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,   262,  1097]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   287,   262,  1097]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290,  5186, 28504]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290,  5186, 28504]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   644,   345, 20143]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   644,   345, 20143]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1312, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1312, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   319,   257, 11443]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   319,   257, 11443]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   307,   262,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   262,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   993,   299,   993]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   993,   299,   993]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  9950,   284,  1394]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  9950,   284,  1394]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1592, 24486,   993]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1592, 24486,   993]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,   262,  4315]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   287,   262,  4315]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257,  1049,  1492]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257,  1049,  1492]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   290, 22891, 17208]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   290, 22891, 17208]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   760,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   760,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1219,   267,  1219]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219,   267,  1219]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   836,   470,   760]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   836,   470,   760]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   909, 16017,   507]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   909, 16017,   507]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   821,   319]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   821,   319]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   261,  2956,   494]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   261,  2956,   494]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  2111]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1101,  2111]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   645,   645,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1309,   340,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1309,   340,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   407,  1865,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   407,  1865,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  8879,   262,   995]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  8879,   262,   995]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   407,  1231,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   407,  1231,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   326]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   326]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   423,   284]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   423,   284]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338, 19217, 19217]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338, 19217, 19217]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   534,  5876]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   534,  5876]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   881,  5875,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   881,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   286,  3511,  9975]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,  3511,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 20374,   523,   890]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 20374,   523,   890]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257, 43937,   640]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257, 43937,   640]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   994,  1312,   467]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   994,  1312,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1510,  1310,  1661]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1510,  1310,  1661]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   995,   561,   466]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   995,   561,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1219, 11752,   645]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219, 11752,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,  1053,  1775]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,  1053,  1775]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1577,   345,  4167]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1577,   345,  4167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3492,   329,  5249]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3492,   329,  5249]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257,  3099,   993]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257,  3099,   993]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,   256,  7167]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   220,   256,  7167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   714,   910, 31983]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   714,   910, 31983]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   531,  2740,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   531,  2740,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   783,   262,   886]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   783,   262,   886]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   468,   587,  2716]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2279,   468,  3421]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2279,   468,  3421]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4953,   319,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4953,   319,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    64,     6,  1900]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    64,     6,  1900]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   307,   523,  1612]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   523,  1612]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,   340,   783]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,   340,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6738,  1582,   271]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6738,  1582,   271]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1549,   766,   340]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1549,   766,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1842,   318, 16903]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1842,   318, 16903]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1842,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1842,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   588,   674,   938]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   674,   938]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   502,  5875,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   502,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 10194]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2951,  4950,  2951]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2951,  4950,  2951]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   655,   910,  3763]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   655,   910,  3763]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   534,  1438, 11752]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   534,  1438, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6765,  2290, 31558]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6765,  2290, 31558]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,  5875,   345]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4245,  5156,  4245]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4245,  5156,  4245]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   673,   338, 10861]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   673,   338, 10861]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4030,  3597,   606]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4030,  3597,   606]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   736,   287,  3240]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   736,   287,  3240]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    72,   128,   247]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    72,   128,   247]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1760,  4917,   334]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1760,  4917,   334]])}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyric = tokenized_lyric[:400]\n",
    "test_lyric = tokenized_lyric[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cde6870977249afad77ddd3bb6844dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m train_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./lyric_results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     11\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_arguments,\n\u001b[0;32m     12\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_lyric,\n\u001b[0;32m     13\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2209\u001b[0m ):\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:3138\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:3161\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3160\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3163\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1327\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1325\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1327\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_arguments = TrainingArguments(\n",
    "    output_dir=\"./lyric_results\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_arguments,\n",
    "    train_dataset=train_lyric,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0      vintage tee brand new phone high heels on cobb...\n1      justin vernon i can see you standing honey wit...\n2      we could leave the christmas lights up 'til ja...\n3      i'm doing good i'm on some new shit been sayin...\n4      i don't like your little games don't like your...\n                             ...                        \n474    drew looks at me i fake a smile so he won't se...\n475    to put it plainly we just couldnt stop writing...\n476    turn wycd on you're on your grunwald back from...\n477    zwrotka  siedzę i patrzę jak czytasz z głową p...\n478    trying just like they say just taking the step...\nName: Lyric, Length: 477, dtype: object is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m----> 2\u001b[0m tokenized_lyric \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tokenized_lyric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3029\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3030\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3035\u001b[0m )\n\u001b[1;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3038\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3039\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3040\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3041\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3042\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3043\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3044\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3045\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3046\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3047\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3048\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3049\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3050\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3051\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3052\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3053\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3054\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3055\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3056\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[1;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    723\u001b[0m     first_ids,\n\u001b[0;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils.py:705\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `is_split_into_words=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0      vintage tee brand new phone high heels on cobb...\n1      justin vernon i can see you standing honey wit...\n2      we could leave the christmas lights up 'til ja...\n3      i'm doing good i'm on some new shit been sayin...\n4      i don't like your little games don't like your...\n                             ...                        \n474    drew looks at me i fake a smile so he won't se...\n475    to put it plainly we just couldnt stop writing...\n476    turn wycd on you're on your grunwald back from...\n477    zwrotka  siedzę i patrzę jak czytasz z głową p...\n478    trying just like they say just taking the step...\nName: Lyric, Length: 477, dtype: object is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "for i in range(size):\n",
    "    text = lyric[i]\n",
    "    tokenized_lyric = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lyric_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./lyric_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   326]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   326]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lyric[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "20\n",
      "21\n",
      "22\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "69\n",
      "70\n",
      "71\n",
      "[9, 10, 17, 20, 22, 25, 32, 40, 48, 69]\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "min_length = 1024\n",
    "to_delete = []\n",
    "print(size)\n",
    "for i in range(size-400):\n",
    "    print(i)\n",
    "    if np.sum(test_lyric[i]['attention_mask'].numpy()) < 100:\n",
    "        # min_length = np.sum(test_lyric[i]['attention_mask'].numpy())\n",
    "        # test_lyric.pop(i)\n",
    "        print(i)\n",
    "        to_delete.append(i)\n",
    "        # size -= 1\n",
    "print(to_delete)\n",
    "for i in range(len(to_delete)):\n",
    "    test_lyric.pop(to_delete[i]-i)\n",
    "print(min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(test_lyric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test():\n",
    "    for i in range(len(test_lyric)):\n",
    "        test_lyric[i]['input_ids'][0] = torch.concat((torch.tensor(np.full((100,), 50256)), test_lyric[i]['input_ids'][0]))[:max_length]\n",
    "        test_lyric[i]['attention_mask'][0] = torch.concat((torch.tensor(np.full((100,), 50256)), test_lyric[i]['attention_mask'][0]))[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[50256, 50256, 50256,  ...,   373, 17717,   259]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  6151,   326]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1183,   651,  1365]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   423,   284]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   523,  1312]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  2051,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   910,   262]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   616,   582]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,  1182,   705]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338, 19217, 19217]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  5998,    78,  3613]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   705, 25587,  1312]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   373,   534,  4004]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,   612,   338]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   338,   534,  5876]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   821,   257]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   881,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3708,  1363,  3436]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262,  4655,   866]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   286,  3511,  9975]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 27516,   788,   345]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 20374,   523,   890]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   262, 15896,  5417]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257, 43937,   640]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616,  2612,   338]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   616,   616, 18854]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338,   262,  1257]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   502,  1453]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,  1392,   284]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   994,  1312,   467]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   484,  6486,   290]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1510,  1310,  1661]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   991, 20406, 14638]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   995,   561,   466]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   607,   588,   326]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   329,   262,  6510]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1219, 11752,   645]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3993,   475,   345]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,  1053,  1775]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1577,   345,   616]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1577,   345,  4167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   356, 11638,   345]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   284,  9480,   866]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   766,   826,   832]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  3492,   329,  5249]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   220,   662,   220]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   257,  3099,   993]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2279,  2073, 17180]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   220,   256,  7167]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 16568,   584,   661]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   714,   910, 31983]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   534,  3815,   329]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  4268,  2279,   783]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   262, 38306,  6129]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,   389,   407]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   531,  2740,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1053,   587,  4203]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   340,  5170,   345]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   783,   262,   886]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   345,  3114,   379]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   531, 23748,   290]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2279,   468,  3421]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  2073,  3387, 17666]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4953,   319,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   703,  1312,  2911]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   481,   307, 12086]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   821,   319,   262]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1312,  1101, 22751]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    64,     6,  1900]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   447,   101,   392]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   307,   523,  1612]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3505,   703,   356]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,   340,   783]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1053,   587,  4203]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   717,  2497,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   645,   220,   220]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1549,   766,   340]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7625,  1022,   534]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1842,   318, 16903]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ..., 36650,    70,  1831]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   345,   523,   881]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1363, 11752,  5156]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1312,  1842,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   257,  4950,  1110]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   588,   674,   938]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   373,   340,  2861]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   502,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   338, 38870,  1107]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ..., 11752, 11752, 10194]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   523,  1312,  3708]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   616, 42858,  3930]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  2951,  4950,  2951]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   284,   262,  2323]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   655,   910,  3763]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   287,   262,  3504]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   534,  1438, 11752]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1183,  1702,   289]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  6765,  2290, 31558]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  6870,   534,  3815]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  7508,  3772,  6709]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  3436,   355,  1312]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   766,  5875,   345]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  7779,   345,  1239]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4245,  5156,  4245]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   607,  1986,   355]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   673,   338, 10861]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   467,  1560,   502]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,   991,   423,   502]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   523,  1312,  3708]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1839,   470,   766]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,   428,  2647,   356]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  4030,  3597,   606]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,  1976,   305,  8482]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,    72,   128,   247]])},\n",
       " {'input_ids': tensor([[50256, 50256, 50256,  ...,    83, 14992,   655]]), 'attention_mask': tensor([[50256, 50256, 50256,  ...,     1,     1,     1]]), 'labels': tensor([[50256, 50256, 50256,  ...,  1760,  4917,   334]])}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_lyric)):\n\u001b[1;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_lyric\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_lyric\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(test_lyric[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[0;32m   1560\u001b[0m         input_ids,\n\u001b[0;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1573\u001b[0m     )\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_greedy_search(\n\u001b[0;32m   1577\u001b[0m         input_ids,\n\u001b[0;32m   1578\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1579\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1580\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1581\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1582\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1583\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1584\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1585\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1587\u001b[0m     )\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2496\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2497\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2498\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2499\u001b[0m )\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1069\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1068\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m-> 1069\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\21700\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for i in range(len(test_lyric)):\n",
    "    output = model.generate(input_ids=test_lyric[i]['input_ids'], attention_mask=test_lyric[i]['attention_mask'], max_new_tokens = 100, num_return_sequences=1)\n",
    "    print(tokenizer.decode(output[0]))\n",
    "    print(tokenizer.decode(test_lyric[i]['labels'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      vintage tee brand new phone high heels on cobb...\n",
       "1      justin vernon i can see you standing honey wit...\n",
       "2      we could leave the christmas lights up 'til ja...\n",
       "3      i'm doing good i'm on some new shit been sayin...\n",
       "4      i don't like your little games don't like your...\n",
       "                             ...                        \n",
       "467    drew looks at me i fake a smile so he won't se...\n",
       "468    to put it plainly we just couldnt stop writing...\n",
       "469    turn wycd on you're on your grunwald back from...\n",
       "470    zwrotka  siedzę i patrzę jak czytasz z głową p...\n",
       "471    trying just like they say just taking the step...\n",
       "Name: Lyric, Length: 472, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./lyric_model\")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "tot_size = len(lyric) - 1\n",
    "print(tot_size)\n",
    "test_size = 72\n",
    "max_token = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.76483331990518 50.15214217450629\n",
      "0.611404440755515 0.2927100245574598\n",
      "42.2319276951886 44.480173519342586\n",
      "49.48259877112261 49.1320506200221\n",
      "16.789369510042775 12.495747658751178\n",
      "20.48597392871792 15.462826578031848\n",
      "9.61473169643006 8.805804131025496\n",
      "43.05215082009055 23.3457734837468\n",
      "19.97762272132689 18.999258213588007\n",
      "19.510893947423178 19.49978970775825\n",
      "27.200068662962394 26.750088613977116\n",
      "9.488815450519796 9.705050805997313\n",
      "10.886751171833199 10.4533552974009\n",
      "43.15399680098916 47.63654824867901\n",
      "18.81299726561037 18.76075279675465\n",
      "13.791258457556518 15.923604479234807\n",
      "14.16521688398309 9.69605324813007\n",
      "5.44512532237439 6.729229114728109\n",
      "18.30193038821825 18.12072763933766\n",
      "8.748236723829718 16.899474521391213\n",
      "27.09355641853726 24.12758632228586\n",
      "19.68458587135496 15.281651207687158\n",
      "8.9660520212405 14.30202095162956\n",
      "27.844115155498905 33.746036000330015\n",
      "10.213745038221145 11.647483659846074\n",
      "15.298188972303544 13.25690617297067\n",
      "14.189527432273586 15.601583726750809\n",
      "16.040306152651073 16.470886589458743\n",
      "8.404010071513696 7.73485274301583\n",
      "11.603226031355085 12.855129468121465\n",
      "2.881661484750454 11.592657008654191\n",
      "76.67136854101996 16.061464408229007\n",
      "33.348193086363366 27.664034883193366\n",
      "8.520768687935497 6.189329672592504\n",
      "9.035826394372027 14.388437734528274\n",
      "11.135194216914844 14.062295536423576\n",
      "2.908726715485709 7.386531401014061\n",
      "18.310230660768834 19.604251806185665\n",
      "5.403914638339836 9.142992054430731\n",
      "76.67136854101996 16.061464408229007\n",
      "10.593753880830175 12.387129178490719\n",
      "32.73796323119122 34.505753474752055\n",
      "26.717494681163217 24.69781877080996\n",
      "11.041139286604896 13.22028305496325\n",
      "38.12415249789598 26.700680072935572\n",
      "49.29588757181454 53.5416842551386\n",
      "76.67136854101996 16.061464408229007\n",
      "25.80848998002717 25.93917005598972\n",
      "29.253158406469936 25.533628296066027\n",
      "76.67136854101996 16.061464408229007\n",
      "45.44570503091996 50.48154709862123\n",
      "76.67136854101996 16.061464408229007\n",
      "40.14813230862217 34.41980964614735\n",
      "26.82342221482547 34.0148486808093\n",
      "76.67136854101996 16.061464408229007\n",
      "10.017704692853247 9.608370475971775\n",
      "39.967925277879274 37.284654854136676\n",
      "24.102159457130572 23.87082550419696\n",
      "23.842480704317282 13.891033264823193\n",
      "45.68380626288216 45.68380626288216\n",
      "23.005567123479732 23.002173847756243\n",
      "76.67136854101996 16.061464408229007\n",
      "76.67136854101996 16.061464408229007\n",
      "22.853730704414268 23.610718894710626\n",
      "40.72654810528153 46.220625446541156\n",
      "22.784723233232032 20.406740708619356\n",
      "48.85415725464038 46.088540283788234\n",
      "15.325336079388455 15.325336079388455\n",
      "16.58800473344212 14.597899898426608\n",
      "10.044429887364863 10.102782000304815\n",
      "32.0221765739327 30.775058486756798\n",
      "14.257216019753866 20.4344664573509\n"
     ]
    }
   ],
   "source": [
    "score_all_model = []\n",
    "score_all_gpt2 = []\n",
    "for i in range(test_size):\n",
    "    ref = [lyric[tot_size - i]]\n",
    "    # tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    prompt = tokenizer(lyric[tot_size - i], max_length=max_token, truncation=True, return_tensors=\"pt\")\n",
    "    output_model = model.generate(input_ids=prompt['input_ids'], attention_mask=prompt['attention_mask'], max_new_tokens=max_token, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    output_gpt2 = gpt2.generate(input_ids=prompt['input_ids'], attention_mask=prompt['attention_mask'], max_new_tokens=max_token, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    score_model = sacrebleu.sentence_bleu(tokenizer.decode(output_model[0]), ref)\n",
    "    score_gpt2 = sacrebleu.sentence_bleu(tokenizer.decode(output_gpt2[0]), ref)\n",
    "    print(score_model.score, score_gpt2.score)\n",
    "    score_all_model.append(score_model.score)\n",
    "    score_all_gpt2.append(score_gpt2.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
